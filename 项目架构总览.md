### 项目总览：AI 论文智能分析助手

* **核心技术栈:** FastAPI, Vue.js, Docker, MongoDB, 向量数据库(Qdrant), PyMuPDF, Transformers
* **开发原则:** MVP优先 (Minimum Viable Product)，快速迭代，每个阶段都有可交付的成果。

---

### 阶段 0：准备与基建 (Preparation & Infrastructure)

**目标:** 搭建好所有必需的开发环境和项目骨架，为后续开发铺平道路。

**主要任务:**
1.  **安装核心软件:** 确保你的电脑上安装了 Python (3.9+), Node.js (18+), VS Code, Git, Docker Desktop。
2.  **项目结构初始化:**
    * 创建一个总的项目文件夹，例如 `paper-pilot`。
    * 在内部创建两个子文件夹：`backend` 和 `frontend`。
3.  **后端环境:**
    * 在 `backend` 文件夹中，创建并激活 Python 虚拟环境 (`python -m venv venv`)。
    * 使用 Git 初始化仓库 (`git init`)，并创建 `.gitignore` 文件，忽略 `venv/`、`__pycache__/` 等。
4.  **前端环境:**
    * 在 `frontend` 文件夹中，使用 Vite 初始化一个 Vue.js + TypeScript 项目 (`npm create vite@latest . -- --template vue-ts`)。
    * 根据提示安装依赖 (`npm install`)。

**✅ 验收标准:**
* 在命令行中可以成功检查到 `python`, `pip`, `node`, `npm`, `git`, `docker` 的版本。
* `backend` 目录下的 Python 虚拟环境可以正常激活和退出。
* `frontend` 目录下的 Vue 默认页面可以通过 `npm run dev` 成功运行并能在浏览器中看到。
* 整个项目已经是一个 Git 仓库，可以进行 commit 操作。

---

### 阶段 1：后端核心骨架 - PDF 解析 API

**目标:** 实现一个最基础的后端服务，验证“接收PDF文件并成功提取文本”这一核心IO能力。**此阶段完全不涉及 AI。**

**主要任务:**
1.  **安装后端依赖:** 在激活的虚拟环境中安装 `fastapi`, `uvicorn`, `python-multipart`, `pymupdf`。
2.  **创建 FastAPI 应用:** 在 `backend` 目录下创建 `main.py`。
3.  **编写上传接口:** 在 `main.py` 中创建一个 API 接口 (Endpoint)，例如 `POST /api/v1/papers/parse`。
4.  **实现解析逻辑:** 该接口接收一个上传的 PDF 文件，使用 `PyMuPDF` 库读取文件内容，提取出所有纯文本。
5.  **返回结果:** 接口将提取出的纯文本作为 JSON 响应返回给客户端。

**✅ 验收标准:**
* 通过 `uvicorn main:app --reload` 启动后端服务。
* 使用 Postman 或 Insomnia 等 API 测试工具，向 `http://127.0.0.1:8000/api/v1/papers/parse` 发送一个 POST 请求，请求体为 `form-data`，其中包含一个 PDF 文件。
* **能够成功收到一个包含该 PDF 全部文本内容的 JSON 响应。** 这证明你的文件上传和文本解析通路已经打通。

---

### 阶段 2：核心AI引擎 - 本地 RAG 脚本验证

**目标:** 在一个独立的 Python 脚本中，验证 RAG (检索增强生成) 流程的可行性。**此阶段不涉及 Web 服务，专注于算法本身。**

**主要任务:**
1.  **安装 AI 依赖:** 安装 `transformers`, `sentence-transformers`, `torch`, `faiss-cpu` (用于本地向量索引)。
2.  **创建测试脚本:** 在 `backend` 目录下创建一个 `test_rag.py`。
3.  **准备数据:** 将一份测试论文的纯文本（可以从阶段1获得）保存为 `sample.txt`。
4.  **编写 RAG 流程:**
    * **加载:** 脚本读取 `sample.txt` 的内容。
    * **分割:** 使用 `RecursiveCharacterTextSplitter` 等工具将文本分割成小块 (Chunks)。
    * **嵌入:** 加载一个开源的 Embedding 模型 (如 `bge-small-zh` from Hugging Face)，将每个文本块转换成向量。
    * **索引:** 使用 `FAISS` 这个库，在内存中为这些向量创建一个索引，用于快速检索。
    * **提问:** 定义一个问题，例如 "What is the main contribution of this paper?"。将问题也转换成向量。
    * **检索:** 在 FAISS 索引中，找到与问题向量最相似的几个文本块。
    * **生成:** 调用一个 LLM API (推荐使用国内的 Kimi Moonshot 或 Zhipu AI 的 API，因为它们对中文支持好、调用方便)。构造一个包含“检索到的文本块”作为上下文的 Prompt，向 LLM 提问。

**✅ 验收标准:**
* 直接在命令行运行 `python test_rag.py`。
* **脚本能够打印出 LLM 返回的、基于 `sample.txt` 内容的、有意义的答案。** 这证明你的核心 AI 问答逻辑是通顺的。

---

### 阶段 3：前后端联调 - MVP 原型诞生

**目标:** 将后端的 AI 引擎与一个极简的前端界面连接起来，跑通完整的用户操作流程。

**主要任务:**
1.  **后端接口化:** 将 `test_rag.py` 中的 RAG 逻辑，封装成一个新的 FastAPI 接口，例如 `POST /api/v1/papers/{paper_id}/ask`。
2.  **前端界面:**
    * 在 Vue 应用中，创建一个非常简单的页面。
    * 页面包含一个文件上传按钮和一个聊天输入框。
    * **不要求美观，只要求功能可用。**
3.  **联调逻辑:**
    * 用户点击上传按钮，前端调用阶段 1 的 `/parse` 接口，获得文本。（此时为了简化，后端可以直接在内存中处理文本和构建索引）。
    * 用户在输入框提问，前端调用新的 `/ask` 接口，将问题发送给后端。
    * 前端接收到后端返回的答案，并显示在页面上。

**✅ 验收标准:**
* 同时启动后端和前端开发服务器。
* 在浏览器中打开你的 Vue 应用，上传一篇论文 PDF，然后在输入框中提问。
* **页面上能够成功显示出 AI 返回的答案。** 这证明你的全流程 MVP 已经成功闭环。

---

### 阶段 4：工程化与持久化 - 引入 Docker 和数据库

**目标:** 将应用容器化，并引入数据库实现数据的持久化存储，让系统变得更健壮、更专业。

**主要任务:**
1.  **数据库选型:** 向量数据库选择 **Qdrant** (比 Milvus 更轻量，更适合入门)。文档数据库就是 **MongoDB**。
2.  **Docker 化:**
    * 在 `backend` 目录下编写 `Dockerfile`，用于打包 FastAPI 应用。
    * 在项目根目录下编写 `docker-compose.yml`，定义三个服务：`backend` (你的应用)、`mongo` (官方镜像)、`qdrant` (官方镜像)。
3.  **后端改造:**
    * 修改后端代码，用 `motor` (MongoDB 的异步驱动) 和 `qdrant-client` 连接到 Docker 容器中的数据库。
    * 修改 `/parse` 接口：解析完文本后，将文本块和元数据存入 MongoDB，将向量存入 Qdrant。
    * 修改 `/ask` 接口：从 Qdrant 中检索，而不是每次都重新构建内存索引。

**✅ 验收标准:**
* 在项目根目录运行 `docker-compose up --build`。
* **三个容器（backend, mongo, qdrant）都能够无错误地成功启动。**
* 使用阶段 3 的前端或 Postman，上传一篇论文。通过数据库客户端（如 Compass for MongoDB）检查，**能看到数据被成功写入 MongoDB 和 Qdrant。**
* 关闭并重启所有容器，再次对**之前上传过**的论文提问（不重新上传），**系统依然能够正确回答**。这证明了数据持久化成功。

---

### 阶段 5：功能增强与体验优化

**目标:** 在稳定的架构上，丰富产品功能，优化用户体验，并尝试模型微调。

**主要任务:**
1.  **前端美化:** 使用 Element Plus 或 Ant Design Vue 等 UI 库，美化界面。引入一个 PDF.js 的封装库，实现前端 PDF 预览。
2.  **后端功能:**
    * 开发“一键速读”接口，返回固定格式的摘要、创新点等。
    * 增加用户系统，实现多用户隔离。
3.  **模型微调 (可选，但含金量高):**
    * 研究 Hugging Face 的 `peft` 库 (特别是 LoRA)。
    * 寻找或构建学术领域的指令微调数据集。
    * 在本地或租用的云 GPU 上，对一个开源模型 (如 Qwen, ChatGLM) 进行微调。
    * 将微调后的模型替换掉之前使用的通用 LLM API，对比效果。

**✅ 验收标准:**
* 前端界面美观易用，PDF 预览流畅。
* “一键速读”等新功能可用。
* 微调后的模型在回答专业问题时，比通用模型更精准、更符合学术语境。

---

### 阶段 6：部署上线

**目标:** 将你的应用部署到云服务器，让任何人都可以通过互联网访问。

**主要任务:**
1.  **购买云服务器:** 购买腾讯云 CVM，安装 Docker 和 Docker Compose。
2.  **代码/镜像上云:**
    * 方式一（简单）：在服务器上 `git pull` 你的代码，然后 `docker-compose up`。
    * 方式二（专业）：将本地构建好的 Docker 镜像推送到腾讯云容器镜像服务 (TCR)，然后在服务器上拉取镜像。
3.  **配置网络与反向代理:**
    * 配置服务器的安全组，开放 80 (HTTP) 和 443 (HTTPS) 端口。
    * 安装 Nginx，并配置反向代理，将来自域名的请求转发到你运行在 Docker 里的 FastAPI 和 Vue 应用。

**✅ 验收标准:**
* **你可以通过你的公网 IP 或域名，在任何地方访问到你的应用，并且所有功能都和本地一样正常。**

